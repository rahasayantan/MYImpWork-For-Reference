{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nnp.random.seed(42)\nimport pandas as pd\nimport string\nimport re\n\nimport gensim\nfrom collections import Counter\nimport pickle\n\nimport tensorflow as tf\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score, f1_score\nfrom sklearn import metrics\n\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Dropout, Conv1D, Embedding, SpatialDropout1D, concatenate\nfrom keras.layers import GRU, LSTM,Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\nfrom keras.layers import CuDNNLSTM, CuDNNGRU\nfrom keras.preprocessing import text, sequence\n\nfrom keras.callbacks import Callback\nfrom keras import optimizers\nfrom keras.layers import Lambda\nfrom keras.callbacks import *\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom nltk.corpus import stopwords\n\nimport os\nos.environ['OMP_NUM_THREADS'] = '4'\n\nimport gc\nfrom keras import backend as K\nfrom sklearn.model_selection import KFold\n\nfrom unidecode import unidecode\n\nimport time\n\neng_stopwords = set(stopwords.words(\"english\"))\n\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"# 1. preprocessing\ntrain = pd.read_csv(\"../input/train.csv\")\ntest = pd.read_csv(\"../input/test.csv\")\n\nprint(\"Train shape : \",train.shape)\nprint(\"Test shape : \",test.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5b42925bff7c0275450d2cb4e2fcbfeef00f5252"},"cell_type":"code","source":"# 1-a. Count non ascii characters\nspecial_character = re.compile(r'[A-Za-z0-9\\.\\-\\?\\!\\,\\#\\@\\% \\'\\/\\\"]',re.IGNORECASE)\ntrain['spl_chars'] = train['question_text'].apply(lambda x: len(special_character.sub('', str(x))))\ntest['spl_chars'] = test['question_text'].apply(lambda x: len(special_character.sub('', str(x))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1b2aa6f947fc1850b8644e461090a83e59337c6c"},"cell_type":"code","source":"#pd.set_option('display.max_colwidth', -1)\n#train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"e173c94e68a61528291cc9473cba37f30847bef8"},"cell_type":"code","source":"#train.loc[train.target==0]['spl_chars'].mean()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"ab6d45bc22305cfe00ec7bdceb8a9764f67bd346"},"cell_type":"code","source":"# 2. remove numbers\ndef clean_numbers(x):\n    x = re.sub('[0-9]{5,}', '#####', x)\n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    return x\n\ntrain['clean_text'] = train['question_text'].apply(lambda x: clean_numbers(str(x)))\ntest['clean_text'] = test['question_text'].apply(lambda x: clean_numbers(str(x)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"91b0a8237489bbc87efb394ce8a83dc9988831fd"},"cell_type":"code","source":"#train['clean_text']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"848325747b9d511eb48b382a2c565324908e5a83"},"cell_type":"code","source":"#3.  remove non-ascii\n\nspecial_character_removal = re.compile(r'[^A-Za-z\\.\\-\\?\\!\\,\\#\\@\\% ]',re.IGNORECASE)\ndef clean_text(x):\n    x_ascii = unidecode(x)\n    x_clean = special_character_removal.sub('',x_ascii)\n    return x_clean\n\ntrain['clean_text'] = train['clean_text'].apply(lambda x: clean_text(str(x)))\ntest['clean_text'] = test['clean_text'].apply(lambda x: clean_text(str(x)))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5c21bc47b5bd5476b7632c43f6dd4546798508b4"},"cell_type":"code","source":"X_train = train['clean_text'].fillna(\"something\").values\ny_train = train.target.values\nX_test = test['clean_text'].fillna(\"something\").values\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"478cbe489de98a6366b82e65ebbbf83264014365"},"cell_type":"code","source":"#X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0a1ddc438df214446749d198ae34fc910ce9e7d9"},"cell_type":"code","source":"def add_features(df):\n    \n    df['comment_text'] = df['clean_text'].fillna('something').apply(lambda x:str(x))\n    df['total_length'] = df['comment_text'].apply(len)\n    df['capitals'] = df['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\n    df['caps_vs_length'] = df['capitals']/df['total_length']\n    df['num_words'] = df.comment_text.str.count('\\S+')\n    df['num_unique_words'] = df['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\n    df['words_vs_unique'] = df['num_unique_words'] / df['num_words']  \n    df['spl_chars_vs_len'] = df['spl_chars']/df['total_length']\n    return df\n\ntrain = add_features(train)\ntest = add_features(test)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"30b2b9829e2c92852aa32f4f00b6f72b351e6440"},"cell_type":"code","source":"train.loc[np.isinf(train.caps_vs_length),'caps_vs_length'] =0\ntrain.loc[np.isinf(train.words_vs_unique),'words_vs_unique'] =0\ntrain.loc[np.isinf(train.spl_chars_vs_len),'spl_chars_vs_len'] =0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d8e0ed4c1f9e2e3ed785155b1c4fd7609e66eed8"},"cell_type":"code","source":"features = train[['caps_vs_length', 'words_vs_unique', 'spl_chars_vs_len']].fillna(0)\ntest_features = test[['caps_vs_length', 'words_vs_unique', 'spl_chars_vs_len']].fillna(0)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"de760831762e47616e9f29d89096d1149506b0f9"},"cell_type":"code","source":"#test[test.num_words>=50].count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"df06403bb00b1af4fe06bca7dc490e8890988d80"},"cell_type":"code","source":"ss = StandardScaler()\nss.fit(np.vstack((features, test_features)))\nfeatures = ss.transform(features)\ntest_features = ss.transform(test_features)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"da25bfaddf3ace9b4319fa5c226813bd429aa4f2"},"cell_type":"code","source":"max_features = 180000\nmaxlen = 50\n\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(X_train) + list(X_test))\nX_train_sequence = tokenizer.texts_to_sequences(X_train)\nX_test_sequence = tokenizer.texts_to_sequences(X_test)\n\nx_train = sequence.pad_sequences(X_train_sequence, maxlen=maxlen)\nx_test = sequence.pad_sequences(X_test_sequence, maxlen=maxlen)\nprint(len(tokenizer.word_index))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"fcb923a725451e2e03a6ae5b04d5be496f3c78a0"},"cell_type":"code","source":"# Load the FastText Web Crawl vectors\nEMBEDDING_FILE_FASTTEXT='../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\nEMBEDDING_FILE_TWITTER='../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\n\n# switching as glove has better support fot this text\nembeddings_index_tw = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE_FASTTEXT,encoding='utf-8'))\nembeddings_index_ft = dict(get_coefs(*o.strip().split(' ')) for o in open(EMBEDDING_FILE_TWITTER,encoding='utf-8'))\n\nspell_model = gensim.models.KeyedVectors.load_word2vec_format(EMBEDDING_FILE_FASTTEXT)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"5e8f9f2bb8a57148e11ed3661d0a834d70a83556"},"cell_type":"code","source":"# This code is  based on: Spellchecker using Word2vec by CPMP\n# https://www.kaggle.com/cpmpml/spell-checker-using-word2vec\n\nwords = spell_model.index2word\n\nw_rank = {}\nfor i,word in enumerate(words):\n    w_rank[word] = i\n\nWORDS = w_rank\n\n# Use fast text as vocabulary\ndef words(text): return re.findall(r'\\w+', text.lower())\n\ndef P(word): \n    \"Probability of `word`.\"\n    # use inverse of rank as proxy\n    # returns 0 if the word isn't in the dictionary\n    return - WORDS.get(word, 0)\n\ndef correction(word): \n    \"Most probable spelling correction for word.\"\n    return max(candidates(word), key=P)\n\ndef candidates(word): \n    \"Generate possible spelling corrections for word.\"\n    return (known([word]) or [word])# or known(edits1(word)) or known(edits2(word))\n\ndef known(words): \n    \"The subset of `words` that appear in the dictionary of WORDS.\"\n    return set(w for w in words if w in WORDS)\n\ndef edits1(word):\n    \"All edits that are one edit away from `word`.\"\n    letters    = 'abcdefghijklmnopqrstuvwxyz'\n    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n    deletes    = [L + R[1:]               for L, R in splits if R]\n    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n    inserts    = [L + c + R               for L, R in splits for c in letters]\n    return set(deletes + transposes + replaces + inserts)\n\ndef edits2(word): \n    \"All edits that are two edits away from `word`.\"\n    return (e2 for e1 in edits1(word) for e2 in edits1(e1))\n\ndef singlify(word):\n    return \"\".join([letter for i,letter in enumerate(word) if i == 0 or letter != word[i-1]])\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1a5245ab137d836d57f56eb0f71392b28f049da7"},"cell_type":"code","source":"WORDS","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9063c74e4b0dcae6bc499b4de88c3dc7ce99e84"},"cell_type":"code","source":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words,601))\n\nsomething_tw = embeddings_index_tw.get(\"something\")\nsomething_ft = embeddings_index_ft.get(\"something\")\n\nsomething = np.zeros((601,))\nsomething[:300,] = something_ft\nsomething[300:600,] = something_tw\nsomething[600,] = 0\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"14bf0e68627109d96ded3a11c511d9c829a5c5c2"},"cell_type":"code","source":"def all_caps(word):\n    return len(word) > 1 and word.isupper()\n\ndef embed_word(embedding_matrix,i,word):\n    embedding_vector_ft = embeddings_index_ft.get(word)\n    if embedding_vector_ft is not None: \n        if all_caps(word):\n            last_value = np.array([1])\n        else:\n            last_value = np.array([0])\n        embedding_matrix[i,:300] = embedding_vector_ft\n        embedding_matrix[i,600] = last_value\n        embedding_vector_tw = embeddings_index_tw.get(word)\n        if embedding_vector_tw is not None:\n            embedding_matrix[i,300:600] = embedding_vector_tw\n\n            \n# Glove vector is used by itself if there is no glove vector but not the other way around.\nfor word, i in word_index.items():\n    \n    if i >= max_features: continue\n        \n    if embeddings_index_ft.get(word) is not None:\n        embed_word(embedding_matrix,i,word)\n    else:\n        # change to > 20 for better score.\n        if len(word) > 26:\n            embedding_matrix[i] = something\n            #print(word)\n        else:\n            word2 = correction(word)\n            #print(word2)\n            if embeddings_index_ft.get(word2) is not None:\n                embed_word(embedding_matrix,i,word2)\n            else:\n                word2 = correction(singlify(word))\n                if embeddings_index_ft.get(word2) is not None:\n                    embed_word(embedding_matrix,i,word2)\n                else:\n                    embedding_matrix[i] = something     \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"746f34d01074e5536b5b85b74084b1a378fffce0"},"cell_type":"code","source":"embedding_matrix.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0d20c805d3c19e56a2418f2e1c90ebcd97999cb5"},"cell_type":"code","source":"del(embeddings_index_tw, embeddings_index_ft); gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"caa275f237ec4a5eceb3f6329e0c40ec6bce78a6"},"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n        self.max_score = 0\n        self.not_better_count = 0\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=1)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))\n            if (score > self.max_score):\n                print(\"*** New High Score (previous: %.6f) \\n\" % self.max_score)\n                model.save_weights(\"best_weights.h5\")\n                self.max_score=score\n                self.not_better_count = 0\n            else:\n                self.not_better_count += 1\n                if self.not_better_count > 3:\n                    print(\"Epoch %05d: early stopping, high score = %.6f\" % (epoch,self.max_score))\n                    self.model.stop_training = True\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9ff70836e3f36bcbd8a1fe096ad275c8af1e5ede"},"cell_type":"code","source":"def get_model(features,clipvalue=1.,num_filters=40,dropout=0.5,embed_size=601):\n    features_input = Input(shape=(features.shape[1],))\n    inp = Input(shape=(maxlen, ))\n    \n    # Layer 1: concatenated fasttext and glove twitter embeddings.\n    x = Embedding(max_features, embed_size, weights=[embedding_matrix], trainable=False)(inp)\n    \n    # Uncomment for best result\n    # Layer 2: SpatialDropout1D(0.5)\n    x = SpatialDropout1D(dropout)(x)\n    \n    # Uncomment for best result\n    # Layer 3: Bidirectional CuDNNLSTM\n    x = Bidirectional(CuDNNLSTM(num_filters, return_sequences=True))(x)\n\n\n    # Layer 4: Bidirectional CuDNNGRU\n    x, x_h, x_c = Bidirectional(CuDNNGRU(num_filters, return_sequences=True, return_state = True))(x)  \n    \n    # Layer 5: A concatenation of the last state, maximum pool, average pool and \n    # two features: \"Unique words rate\" and \"Rate of all-caps words\"\n    avg_pool = GlobalAveragePooling1D()(x)\n    max_pool = GlobalMaxPooling1D()(x)\n    \n    x = concatenate([avg_pool, x_h, max_pool,features_input])\n    \n    # Layer 6: output dense layer.\n    outp = Dense(1, activation=\"sigmoid\")(x)\n\n    model = Model(inputs=[inp,features_input], outputs=outp)\n    adam = optimizers.adam(clipvalue=clipvalue)\n    model.compile(loss='binary_crossentropy',\n                  optimizer=adam,\n                  metrics=['accuracy'])\n    return model\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"48c3d67be5fe44871b5b6c926740f1d93761e9db"},"cell_type":"code","source":"model = get_model(features)\n\nbatch_size = 512\n\n# Used epochs=100 with early exiting for best score.\nepochs = 7\ngc.collect()\nK.clear_session()\n\n# Change to 5\nnum_folds = 5 #number of folds\n\ny_test = np.zeros((test.shape[0],1))\n\n# Uncomment for out-of-fold predictions\nscores = []\noof_predict = np.zeros((train.shape[0],1))\n\nkf = KFold(n_splits=num_folds, shuffle=True, random_state=239)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d0fe5c5f79a3857df30bfbfda75ed44fbbddf3e2"},"cell_type":"code","source":"def f1_smart(y_true, y_pred):\n    args = np.argsort(y_pred)\n    tp = y_true.sum()\n    fs = (tp - np.cumsum(y_true[args[:-1]])) / np.arange(y_true.shape[0] + tp - 1, tp, -1)\n    res_idx = np.argmax(fs)\n    return 2 * fs[res_idx], (y_pred[args[res_idx]] + y_pred[args[res_idx + 1]]) / 2\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f77fd6268d31e91d85af3b9714398393737de8df"},"cell_type":"code","source":"bestscore = []\n\nfor train_index, test_index in kf.split(x_train):\n    filepath=\"weights_best.h5\"\n    kfold_y_train,kfold_y_test = y_train[train_index], y_train[test_index]\n    kfold_X_train = x_train[train_index]\n    kfold_X_features = features[train_index]\n    kfold_X_valid = x_train[test_index]\n    kfold_X_valid_features = features[test_index] \n    \n    gc.collect()\n    K.clear_session()\n    \n    model = get_model(features)\n    \n    #ra_val = RocAucEvaluation(validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test), interval = 1)\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.0001, verbose=2)\n    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n    \n    if i == 0:print(model.summary()) \n    \n    model.fit([kfold_X_train,kfold_X_features], kfold_y_train, batch_size=batch_size, epochs=epochs, verbose=1,\n              validation_data=([kfold_X_valid,kfold_X_valid_features], kfold_y_test),\n              callbacks = [checkpoint, reduce_lr, earlystopping])#ra_val, \n    gc.collect()\n    \n    #model.load_weights(bst_model_path)\n    model.load_weights(filepath)\n    \n    y_test += model.predict([x_test,test_features], batch_size=1024,verbose=1) / num_folds\n    \n    gc.collect()\n    # uncomment for out of fold predictions\n    oof_predict[test_index] = model.predict([kfold_X_valid, kfold_X_valid_features],batch_size=batch_size, verbose=1)\n    cv_score = roc_auc_score(kfold_y_test, oof_predict[test_index])\n\n    f1, threshold = f1_smart(np.squeeze(kfold_y_test), np.squeeze(oof_predict[test_index]))\n    print('Optimal F1: {:.4f} at threshold: {:.4f}'.format(f1, threshold))    \n    bestscore.append(threshold)\n    scores.append(cv_score)\n    print('score: ',cv_score)\n\nprint(\"Done\")\nprint('Total CV score is {}'.format(np.mean(scores)))    \n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"2361998e3847bc4a874a46cc0b27b510826a7ded"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"0f3013249d7f16ca1fa6dea824a4c399046715a6"},"cell_type":"code","source":"from sklearn.metrics import f1_score\ndef threshold_search(y_true, y_proba):\n    best_threshold =0\n    best_score = 0\n    for threshold in [i * 0.01 for i in range(100)]:\n        score = f1_score(y_true=y_true, y_pred=y_proba > threshold)\n        if score > best_score:\n            best_threshold = threshold\n            best_score = score\n    search_result = {'threshold': best_threshold, 'f1': best_score}\n    return search_result\n\nsearch_result = threshold_search(y_train, oof_predict)\nprint(search_result)\n\nprint(\"Mean of Best Score ::: {}\".format(np.mean(bestscore)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d9850e3a5ff5045d549fe66cc873b037f5c05d47"},"cell_type":"code","source":"#sum((y_test>.38).reshape(-1)==1)\n#sum(y_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"72a6df50370a51a36fbcb3203f8cd33eb8d2e0e7"},"cell_type":"code","source":"sub = test[['qid']]\ny_test = y_test.reshape((-1, 1))\npred_test_y = (y_test>search_result['threshold']).astype(int)#np.mean(bestscore)\nsub['prediction'] = pred_test_y\nsub.to_csv(\"submission.csv\", index=False)                                   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a86ceaa0021086bc57f4deff49cbfeeb2828ebdf"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}