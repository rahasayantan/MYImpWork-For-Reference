{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import linalg as slin\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPM = np.array([\n",
    "    [0.8, 0.1, 0.1, 0],\n",
    "    [0, 0.7, 0.2, 0.1],\n",
    "    [0, 0, 0.7, 0.3],\n",
    "    [0, 0, 0, 1]\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = [0,1,2,3]\n",
    "action = [0,1,2]\n",
    "\n",
    "imm_reward = np.array([\n",
    "    [20000],\n",
    "    [16000],\n",
    "    [12000],\n",
    "    [5000]\n",
    "]).reshape(-1)\n",
    "\n",
    "action_cost = np.array([\n",
    "    [0],\n",
    "    [2000],\n",
    "    [10000]\n",
    "])\n",
    "\n",
    "policy_state_transition = [(0,0,0), (1,0,1), (1,2,0), (2,0,2), (2,1,1), (2,2,0), (3,0,3), (3,1,1), (3,2,0)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the immediate reward matrix for states and actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[20000.],\n",
       "        [    0.],\n",
       "        [    0.]],\n",
       "\n",
       "       [[16000.],\n",
       "        [    0.],\n",
       "        [10000.]],\n",
       "\n",
       "       [[12000.],\n",
       "        [14000.],\n",
       "        [10000.]],\n",
       "\n",
       "       [[ 5000.],\n",
       "        [14000.],\n",
       "        [10000.]]])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def state_action_imm_rew(policy_state_transition, imm_reward, action_cost):\n",
    "\n",
    "    state_action = np.zeros((len(state),len(action),1))\n",
    "    state_action_imm_rew = np.array(state_action.copy())\n",
    "    state_action_imm_rew\n",
    "    for s, a, s1 in policy_state_transition:\n",
    "        if s == 0:\n",
    "            state_action_imm_rew[s,a,:] = imm_reward[s1] - action_cost[a]\n",
    "        elif (s == 1) & (a == 1):\n",
    "            state_action_imm_rew[s,a,:] = 0\n",
    "        elif (s == 1) & (a == 2):\n",
    "            state_action_imm_rew[s,a,:] = imm_reward[s1]- action_cost[a]\n",
    "        else:\n",
    "            state_action_imm_rew[s,a,:] = imm_reward[s1] - action_cost[a]\n",
    "    return state_action_imm_rew\n",
    "        \n",
    "        \n",
    "state_action_imm_rew = state_action_imm_rew(policy_state_transition, imm_reward, action_cost)\n",
    "state_action_imm_rew            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs to test Policy and reward -- Having challenge to incorporate for period for discount factoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Transition Matrix based on Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_value_function(TPM, policy_state, reward):\n",
    "    transitions = []\n",
    "    rewards = []\n",
    "    for p in policy_state:\n",
    "        transitions.append(TPM[p[2]])\n",
    "        rewards.append(state_action_imm_rew[p[0],p[1]])\n",
    "    new_mat =np.array(transitions) * discount ** (1)\n",
    "    #for i in np.arange(future_period):\n",
    "    #    if i > 0:\n",
    "    #        new_mat += np.array(transitions) * discount ** (i+1)\n",
    "    PP1 = new_mat * -1 + np.identity(TPM.shape[0])\n",
    "    print(rewards)\n",
    "    value_fn = slin.solve(PP1, rewards)\n",
    "    return value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([20000.]), array([16000.]), array([14000.]), array([14000.])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[326850.],\n",
       "       [308600.],\n",
       "       [306600.],\n",
       "       [306600.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount = 0.95\n",
    "future_period = 1 # not yet ready to solve for infinite sequence\n",
    "policy = [0,0,1,1] # what policy we are defining # read from source or input entered by hand\n",
    "policy_state = [(0,0,0), (1,0,1), (2,1,1), (3,1,1)] # this has to be created by hand \n",
    "#/ input should be read from source # Format: initial State, Action, Current State\n",
    "\n",
    "value_fn = get_value_function(TPM, policy_state, state_action_imm_rew)\n",
    "\n",
    "value_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Inputs -- Need to first test for only changed policy, then if new policy > than old policy, we recompute (Policy Iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1248649.9999999981"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_old = np.sum(value_fn)\n",
    "t_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "discount = 0.95\n",
    "future_period = 1 # not yet ready to solve for infinite sequence\n",
    "policy = [0,0,1,2] # what policy we are defining # read from source or input entered by hand\n",
    "policy_state = [(0,0,0), (1,0,1), (2,1,1), (3,2,0)] # this has to be created by hand / input should be read from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([20000.]), array([16000.]), array([14000.]), array([10000.])]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[337895.52238806],\n",
       "       [322552.23880597],\n",
       "       [320552.23880597],\n",
       "       [327895.52238806]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_fn = get_value_function(TPM, policy_state,state_action_imm_rew)\n",
    "value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, False]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_state1 = [(0,0,0), (1,0,1), (2,1,1), (3,1,1)]\n",
    "policy_state2 = [(0,0,0), (1,0,1), (2,1,1), (3,2,0)]\n",
    "\n",
    "changed_policy=[policy_state1[i] == policy_state2[i] for i in range(len(policy_state1))]\n",
    "changed_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "changed_index = np.where(np.array(changed_policy)!=True)[0][0]\n",
    "changed_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[326850.],\n",
       "       [308600.],\n",
       "       [306600.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_fn1 = value_fn[0:changed_index:]\n",
    "value_fn1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Policy is better. hence recomputing.\n",
      "[array([20000.]), array([16000.]), array([14000.]), array([10000.])]\n"
     ]
    }
   ],
   "source": [
    "transitions = []\n",
    "rewards = []\n",
    "p,q,r = policy_state2[changed_index]\n",
    "\n",
    "val_new = (state_action_imm_rew[p,q] + discount * TPM[r, list(set(state) - set([changed_index]))].dot(value_fn1)) / (1 - TPM[r, changed_index])\n",
    "t_new = np.sum(value_fn1) + val_new\n",
    "if t_new > t_old:\n",
    "    print(\"New Policy is better. hence recomputing.\")\n",
    "    policy_state = [(0,0,0), (1,0,1), (2,1,1), (3,2,0)]\n",
    "    value_fn = get_value_function(TPM, policy_state,state_action_imm_rew)\n",
    "value_fn\n",
    "t_old = np.sum(value_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LP formulation for MDP -- Need to generalize the equation formation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mdp01:\n",
       "MINIMIZE\n",
       "None\n",
       "SUBJECT TO\n",
       "_C1: 0.24 dv_0 - 0.095 dv_1 - 0.095 dv_2 >= 20000\n",
       "\n",
       "_C2: 0.335 dv_1 - 0.19 dv_2 - 0.095 dv_3 >= 16000\n",
       "\n",
       "_C3: - 0.76 dv_0 + 0.905 dv_1 - 0.095 dv_2 >= 10000\n",
       "\n",
       "_C4: 0.335 dv_2 - 0.285 dv_3 >= 12000\n",
       "\n",
       "_C5: - 0.665 dv_1 + 0.81 dv_2 - 0.095 dv_3 >= 14000\n",
       "\n",
       "_C6: - 0.76 dv_0 - 0.095 dv_1 + 0.905 dv_2 >= 10000\n",
       "\n",
       "_C7: 0.05 dv_3 >= 5000\n",
       "\n",
       "_C8: - 0.665 dv_1 - 0.19 dv_2 + 0.905 dv_3 >= 14000\n",
       "\n",
       "_C9: - 0.76 dv_0 - 0.095 dv_1 - 0.095 dv_2 + dv_3 >= 10000\n",
       "\n",
       "VARIABLES\n",
       "dv_0 Continuous\n",
       "dv_1 Continuous\n",
       "dv_2 Continuous\n",
       "dv_3 Continuous"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pulp import *\n",
    "# initialize the model\n",
    "prob = LpProblem(\"mdp01\", LpMinimize)\n",
    "\n",
    "#########\n",
    "policy_state_transition = [(0,0,0), (1,0,1), (1,2,0), (2,0,2), (2,1,1), (2,2,0), (3,0,3), (3,1,1), (3,2,0)]\n",
    "\n",
    "transitions = []\n",
    "rewards = []\n",
    "for p in policy_state_transition:\n",
    "    transitions.append(TPM[p[2]])\n",
    "    rewards.append(state_action_imm_rew[p[0],p[1]])\n",
    "new_mat =np.array(transitions) * discount ** (1)\n",
    "\n",
    "T = len(state)\n",
    "# ---------------------\n",
    "# VARIABLES\n",
    "# ---------------------\n",
    "\n",
    "dv = LpVariable.dicts(\"dv\", range(0, T), 0, None)\n",
    "\n",
    "# Constraints\n",
    "\n",
    "prob += dv[0] >= lpSum([new_mat[0, i] * dv[i] for i in np.arange(T)]) + rewards[0]\n",
    "\n",
    "prob += dv[1] >= lpSum([new_mat[1, i] * dv[i] for i in np.arange(T)]) + rewards[1]\n",
    "prob += dv[1] >= lpSum([new_mat[2, i] * dv[i] for i in np.arange(T)]) + rewards[2]\n",
    "\n",
    "prob += dv[2] >= lpSum([new_mat[3, i] * dv[i] for i in np.arange(T)]) + rewards[3]\n",
    "prob += dv[2] >= lpSum([new_mat[4, i] * dv[i] for i in np.arange(T)]) + rewards[4]\n",
    "prob += dv[2] >= lpSum([new_mat[5, i] * dv[i] for i in np.arange(T)]) + rewards[5]\n",
    "\n",
    "prob += dv[3] >= lpSum([new_mat[6, i] * dv[i] for i in np.arange(T)]) + rewards[6]\n",
    "prob += dv[3] >= lpSum([new_mat[7, i] * dv[i] for i in np.arange(T)]) + rewards[7]\n",
    "prob += dv[3] >= lpSum([new_mat[8, i] * dv[i] for i in np.arange(T)]) + rewards[8]\n",
    "\n",
    "\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "dv\n",
      "362000.0\n",
      "352000.0\n",
      "352000.0\n",
      "352000.0\n",
      "Objective 1418000.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Objective function\n",
    "prob += dv[0] + dv[1] + dv[2] + dv[3], \"Objective\"\n",
    "    \n",
    "    \n",
    "prob.writeLP(\"mdp01.lp\")\n",
    "    \n",
    "status = prob.solve(GLPK(options=[\"--ranges\",\"mdp01.sen\"]))\n",
    "print(status)\n",
    "\n",
    "#print the result\n",
    "print(\"dv\")\n",
    "for i in range(0, T):\n",
    "    print(dv[i].value())\n",
    "    \n",
    "print(\"Objective\", value(prob.objective))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load mdp01.sen\n",
    "GLPK 4.65 - SENSITIVITY ANALYSIS REPORT                                                                         Page   1\n",
    "\n",
    "Problem:    \n",
    "Objective:  Objective = 1418000 (MINimum)\n",
    "\n",
    "   No. Row name     St      Activity         Slack   Lower bound       Activity      Obj coef  Obj value at Limiting\n",
    "                                          Marginal   Upper bound          range         range   break point variable\n",
    "------ ------------ -- ------------- ------------- -------------  ------------- ------------- ------------- ------------\n",
    "     1 _C1          NL   20000.00000        .        20000.00000    17894.73684     -61.80000   1.28789e+06 _C2\n",
    "                                          61.80000          +Inf           +Inf          +Inf          +Inf\n",
    "\n",
    "     2 _C2          BS   17600.00000   -1600.00000   16000.00000    20315.78947     -20.00000     1.066e+06 _C3\n",
    "                                            .               +Inf    15733.33333      10.52632   1.60326e+06 _C9\n",
    "\n",
    "     3 _C3          NL   10000.00000        .        10000.00000     6279.06977      -8.60000     1.386e+06 _C2\n",
    "                                           8.60000          +Inf    16315.78947          +Inf   1.47232e+06 _C5\n",
    "\n",
    "     4 _C4          BS   17600.00000   -5600.00000   12000.00000    24842.10526     -20.00000     1.066e+06 _C6\n",
    "                                            .               +Inf    12800.00000       3.50877   1.47975e+06 _C9\n",
    "\n",
    "     5 _C5          BS   17600.00000   -3600.00000   14000.00000    32842.10526      -9.50276   1.25075e+06 _C6\n",
    "                                            .               +Inf    16000.00000      10.52632   1.60326e+06 _C9\n",
    "\n",
    "     6 _C6          NL   10000.00000        .        10000.00000     6022.09945      -8.60000   1.38379e+06 _C5\n",
    "                                           8.60000          +Inf    26842.10526          +Inf   1.56284e+06 _C2\n",
    "\n",
    "     7 _C7          BS   17600.00000  -12600.00000    5000.00000    18442.10526     -20.00000     1.066e+06 _C9\n",
    "                                            .               +Inf    17600.00000          +Inf          +Inf\n",
    "\n",
    "     8 _C8          BS   17600.00000   -3600.00000   14000.00000    32842.10526      -1.10497   1.39855e+06 _C9\n",
    "                                            .               +Inf    14000.00000      15.08772   1.68354e+06 _C3\n",
    "\n",
    "     9 _C9          NL   10000.00000        .        10000.00000     6022.09945      -1.00000   1.41402e+06 _C8\n",
    "                                           1.00000          +Inf    26842.10526          +Inf   1.43484e+06 _C2\n",
    "\n",
    "GLPK 4.65 - SENSITIVITY ANALYSIS REPORT                                                                         Page   2\n",
    "\n",
    "Problem:    \n",
    "Objective:  Objective = 1418000 (MINimum)\n",
    "\n",
    "   No. Column name  St      Activity      Obj coef   Lower bound       Activity      Obj coef  Obj value at Limiting\n",
    "                                          Marginal   Upper bound          range         range   break point variable\n",
    "------ ------------ -- ------------- ------------- -------------  ------------- ------------- ------------- ------------\n",
    "     1 dv_0         BS  362000.00000       1.00000        .                +Inf      -2.81481   37037.03704 _C1\n",
    "                                            .               +Inf   362000.00000          +Inf          +Inf\n",
    "\n",
    "     2 dv_1         BS  352000.00000       1.00000        .        370315.78947      -1.96552  374137.93103 _C3\n",
    "                                            .               +Inf   352000.00000          +Inf          +Inf\n",
    "\n",
    "     3 dv_2         BS  352000.00000       1.00000        .        400842.10526      -1.96552  374137.93103 _C6\n",
    "                                            .               +Inf   352000.00000          +Inf          +Inf\n",
    "\n",
    "     4 dv_3         BS  352000.00000       1.00000        .        368842.10526        .          1.066e+06 _C9\n",
    "                                            .               +Inf   352000.00000          +Inf          +Inf\n",
    "\n",
    "End of report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
